#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¸¸æˆAIæ§åˆ¶å™¨
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
import random
import os
from typing import Dict, List, Tuple, Any

class DQNNetwork(nn.Module):
    """æ·±åº¦Qç½‘ç»œ - ç”¨äºå¼ºåŒ–å­¦ä¹ """
    
    def __init__(self, state_size=50, action_size=20, hidden_size=128):
        super(DQNNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            module.bias.data.fill_(0.01)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

class RLGameAI:
    """åŸºäºå¼ºåŒ–å­¦ä¹ çš„æ¸¸æˆAIæ§åˆ¶å™¨"""
    
    def __init__(self, state_size=50, action_size=20, model_path=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¤– RLæ¸¸æˆAIä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # ç½‘ç»œå‚æ•°
        self.state_size = state_size
        self.action_size = action_size
        self.hidden_size = 128
        
        # ç¥ç»ç½‘ç»œ
        self.q_network = DQNNetwork(state_size, action_size, self.hidden_size).to(self.device)
        self.target_network = DQNNetwork(state_size, action_size, self.hidden_size).to(self.device)
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)
        
        # å¼ºåŒ–å­¦ä¹ å‚æ•°
        self.epsilon = 1.0          # æ¢ç´¢ç‡
        self.epsilon_min = 0.01     # æœ€å°æ¢ç´¢ç‡
        self.epsilon_decay = 0.995  # æ¢ç´¢ç‡è¡°å‡
        self.gamma = 0.95           # æŠ˜æ‰£å› å­
        self.learning_rate = 0.001  # å­¦ä¹ ç‡
        
        # ç»éªŒå›æ”¾
        self.memory = deque(maxlen=10000)
        self.batch_size = 32
        
        # è®­ç»ƒå‚æ•°
        self.update_target_every = 100  # æ¯100æ­¥æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.step_count = 0
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
            print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒRLæ¨¡å‹: {model_path}")
    
    def encode_game_state(self, game_state: Dict[str, Any]) -> torch.Tensor:
        """å°†æ¸¸æˆçŠ¶æ€ç¼–ç ä¸ºç¥ç»ç½‘ç»œè¾“å…¥"""
        features = []
        
        # ç©å®¶çŠ¶æ€
        features.extend([
            game_state.get('player_health', 100) / 100.0,
            game_state.get('player_score', 0) / 1000.0,
            game_state.get('enemies_killed', 0) / 50.0,
            game_state.get('survival_time', 0) / 300.0,
            game_state.get('player_position', {}).get('x', 400) / 800.0,
            game_state.get('player_position', {}).get('y', 300) / 600.0,
        ])
        
        # æ•ŒæœºçŠ¶æ€
        enemies = game_state.get('enemies', [])
        features.extend([
            len(enemies) / 20.0,
            sum(e.get('speed', 5) for e in enemies) / max(len(enemies), 1) / 10.0,
            sum(e.get('health', 1) for e in enemies) / max(len(enemies), 1) / 5.0,
        ])
        
        # é“å…·çŠ¶æ€
        power_ups = game_state.get('power_ups', [])
        features.extend([
            len(power_ups) / 10.0,
            sum(1 for p in power_ups if p.get('type') == 'health') / max(len(power_ups), 1),
            sum(1 for p in power_ups if p.get('type') == 'shield') / max(len(power_ups), 1),
        ])
        
        # æ¸¸æˆç¯å¢ƒ
        features.extend([
            game_state.get('ai_difficulty', 1.0) / 2.0,
            game_state.get('player_performance', 0.5),
            game_state.get('frame_count', 0) / 1000.0,
        ])
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(features) < self.state_size:
            features.append(0.0)
        
        return torch.FloatTensor(features[:self.state_size]).to(self.device)
    
    def select_action(self, game_state: Dict[str, Any]) -> int:
        """é€‰æ‹©åŠ¨ä½œï¼ˆepsilon-greedyç­–ç•¥ï¼‰"""
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.randrange(self.action_size)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            state = self.encode_game_state(game_state)
            with torch.no_grad():
                q_values = self.q_network(state)
            return q_values.argmax().item()
    
    def decode_action(self, action_id: int) -> Dict[str, Any]:
        """å°†åŠ¨ä½œIDè§£ç ä¸ºæ¸¸æˆå‚æ•°"""
        # åŠ¨ä½œç©ºé—´å®šä¹‰ï¼š20ä¸ªä¸åŒçš„æ¸¸æˆé…ç½®
        actions = [
            # æ³¢æ¬¡æ”»å‡»æ¨¡å¼
            {'type': 'wave', 'wave_count': 3, 'enemies_per_wave': 5, 'wave_delay': 2.0},
            {'type': 'wave', 'wave_count': 5, 'enemies_per_wave': 8, 'wave_delay': 3.0},
            {'type': 'wave', 'wave_count': 7, 'enemies_per_wave': 12, 'wave_delay': 4.0},
            {'type': 'wave', 'wave_count': 8, 'enemies_per_wave': 15, 'wave_delay': 5.0},
            
            # èºæ—‹æ”»å‡»æ¨¡å¼
            {'type': 'spiral', 'spiral_arms': 2, 'enemies_per_arm': 8, 'spiral_tightness': 0.5},
            {'type': 'spiral', 'spiral_arms': 3, 'enemies_per_arm': 10, 'spiral_tightness': 1.0},
            {'type': 'spiral', 'spiral_arms': 4, 'enemies_per_arm': 15, 'spiral_tightness': 1.5},
            {'type': 'spiral', 'spiral_arms': 6, 'enemies_per_arm': 20, 'spiral_tightness': 2.0},
            
            # ç¼–é˜Ÿæ”»å‡»æ¨¡å¼
            {'type': 'formation', 'formation_type': 'v', 'formation_size': 3, 'formation_spacing': 50},
            {'type': 'formation', 'formation_type': 'square', 'formation_size': 4, 'formation_spacing': 80},
            {'type': 'formation', 'formation_type': 'triangle', 'formation_size': 5, 'formation_spacing': 100},
            {'type': 'formation', 'formation_type': 'diamond', 'formation_size': 6, 'formation_spacing': 120},
            
            # éšæœºæ”»å‡»æ¨¡å¼
            {'type': 'random', 'enemy_count': 20, 'spawn_interval': 0.5},
            {'type': 'random', 'enemy_count': 30, 'spawn_interval': 1.0},
            {'type': 'random', 'enemy_count': 40, 'spawn_interval': 1.5},
            {'type': 'random', 'enemy_count': 50, 'spawn_interval': 2.0},
            
            # æ··æ²Œæ”»å‡»æ¨¡å¼
            {'type': 'chaos', 'enemy_count': 25, 'spawn_interval': 0.8},
            {'type': 'chaos', 'enemy_count': 35, 'spawn_interval': 1.2},
            {'type': 'chaos', 'enemy_count': 45, 'spawn_interval': 1.8},
            {'type': 'chaos', 'enemy_count': 55, 'spawn_interval': 2.5},
        ]
        
        # æ·»åŠ éšæœºå‚æ•°å˜åŒ–
        action = actions[action_id].copy()
        if 'enemy_speed' not in action:
            action['enemy_speed'] = random.randint(1, 5)
        if 'enemy_health' not in action:
            action['enemy_health'] = random.randint(1, 5)
        if 'enemy_behavior' not in action:
            action['enemy_behavior'] = random.choice(['straight', 'zigzag', 'circle', 'chase', 'evade'])
        
        return action
    
    def remember(self, state: Dict[str, Any], action: int, reward: float, 
                next_state: Dict[str, Any], done: bool):
        """å­˜å‚¨ç»éªŒåˆ°å›æ”¾ç¼“å†²åŒº"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        """ä»ç»éªŒå›æ”¾ä¸­å­¦ä¹ """
        if len(self.memory) < self.batch_size:
            return
        
        # éšæœºé‡‡æ ·ç»éªŒ
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        # è½¬æ¢ä¸ºå¼ é‡
        states = torch.stack([self.encode_game_state(s) for s in states])
        next_states = torch.stack([self.encode_game_state(s) for s in next_states])
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        with torch.no_grad():
            next_q_values = self.target_network(next_states).max(1)[0]
            target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # è®¡ç®—æŸå¤±
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        # æ›´æ–°æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        # æ›´æ–°ç›®æ ‡ç½‘ç»œ
        self.step_count += 1
        if self.step_count % self.update_target_every == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        return loss.item()
    
    def act(self, game_state: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®å½“å‰æ¸¸æˆçŠ¶æ€é€‰æ‹©å¹¶æ‰§è¡ŒåŠ¨ä½œ"""
        # é€‰æ‹©åŠ¨ä½œ
        action_id = self.select_action(game_state)
        
        # è§£ç åŠ¨ä½œ
        action_params = self.decode_action(action_id)
        
        return action_params
    
    def learn_from_game(self, game_states: List[Dict[str, Any]], 
                        actions: List[int], 
                        game_outcome: Dict[str, Any]):
        """ä»å®Œæ•´æ¸¸æˆè¿‡ç¨‹ä¸­å­¦ä¹ """
        if len(game_states) < 2:
            return
        
        # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„å¥–åŠ±
        total_reward = self._calculate_game_reward(game_outcome)
        reward_per_step = total_reward / len(game_states)
        
        # ä¸ºæ¯ä¸ªçŠ¶æ€åˆ†é…å¥–åŠ±
        for i, (state, action) in enumerate(zip(game_states, actions)):
            # è¶Šæ¥è¿‘æ¸¸æˆç»“æŸï¼Œå¥–åŠ±è¶Šé«˜
            step_reward = reward_per_step * (i + 1) / len(game_states)
            
            # ç¡®å®šä¸‹ä¸€ä¸ªçŠ¶æ€
            next_state = game_states[i + 1] if i + 1 < len(game_states) else state
            done = (i + 1 == len(game_states))
            
            # å­˜å‚¨ç»éªŒ
            self.remember(state, action, step_reward, next_state, done)
        
        # è¿›è¡Œå­¦ä¹ 
        loss = self.replay()
        if loss is not None:
            print(f"ğŸ¤– RLå­¦ä¹ å®Œæˆ - æŸå¤±: {loss:.4f}, æ¢ç´¢ç‡: {self.epsilon:.3f}")
    
    def _calculate_game_reward(self, game_outcome: Dict[str, Any]) -> float:
        """è®¡ç®—æ¸¸æˆæ€»å¥–åŠ±"""
        reward = 0.0
        
        # ç”Ÿå­˜æ—¶é—´å¥–åŠ±
        survival_time = game_outcome.get('survival_time', 0)
        reward += min(survival_time / 60.0, 1.0) * 20
        
        # å‡»æ€å¥–åŠ±
        enemies_killed = game_outcome.get('enemies_killed', 0)
        reward += min(enemies_killed / 20.0, 1.0) * 15
        
        # åˆ†æ•°å¥–åŠ±
        score = game_outcome.get('score', 0)
        reward += min(score / 1000.0, 1.0) * 10
        
        # æƒ©ç½šé¡¹
        damage_taken = game_outcome.get('damage_taken', 0)
        reward -= min(damage_taken / 100.0, 1.0) * 5
        
        return reward
    
    def save_model(self, model_path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'q_network': self.q_network.state_dict(),
            'target_network': self.target_network.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'step_count': self.step_count,
        }, model_path)
        print(f"âœ… RLæ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)
        self.q_network.load_state_dict(checkpoint['q_network'])
        self.target_network.load_state_dict(checkpoint['target_network'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.epsilon = checkpoint.get('epsilon', self.epsilon)
        self.step_count = checkpoint.get('step_count', self.step_count)
        print(f"âœ… RLæ¨¡å‹å·²ä» {model_path} åŠ è½½")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºRLæ¸¸æˆAI
    rl_ai = RLGameAI()
    
    # æ¨¡æ‹Ÿæ¸¸æˆè¿‡ç¨‹
    game_states = []
    actions = []
    
    # æ¨¡æ‹Ÿå‡ ä¸ªæ¸¸æˆçŠ¶æ€
    for i in range(10):
        game_state = {
            'player_health': 100 - i * 5,
            'player_score': i * 100,
            'enemies_killed': i * 2,
            'survival_time': i * 10,
            'player_position': {'x': 400, 'y': 300},
            'enemies': [{'speed': 3, 'health': 2}] * (5 + i),
            'power_ups': [{'type': 'health'}] * (2 - i // 5),
            'ai_difficulty': 1.0 + i * 0.1,
            'player_performance': 0.8 - i * 0.05,
            'frame_count': i * 60
        }
        game_states.append(game_state)
        
        # é€‰æ‹©åŠ¨ä½œ
        action = rl_ai.select_action(game_state)
        actions.append(action)
        
        print(f"æ­¥éª¤ {i}: åŠ¨ä½œ {action} -> {rl_ai.decode_action(action)}")
    
    # æ¨¡æ‹Ÿæ¸¸æˆç»“æœ
    game_outcome = {
        'survival_time': 100,
        'enemies_killed': 20,
        'score': 1000,
        'damage_taken': 50
    }
    
    # å­¦ä¹ 
    rl_ai.learn_from_game(game_states, actions, game_outcome)
    
    # ä¿å­˜æ¨¡å‹
    rl_ai.save_model('./models/rl_game_ai.pth')
