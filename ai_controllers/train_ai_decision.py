#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå†³ç­–ç³»ç»Ÿè®­ç»ƒè„šæœ¬
è®­ç»ƒAIæ§åˆ¶å™¨è¿›è¡Œæ™ºèƒ½å†³ç­–ï¼Œæ›¿ä»£è§„åˆ™AI
"""

import os
import sys
import time
import numpy as np
from typing import Dict, List, Any
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from stable_baselines3 import PPO, DQN, A2C
    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
    from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
    from stable_baselines3.common.monitor import Monitor
    SB3_AVAILABLE = True
    print("âœ… Stable Baselines3 å¯ç”¨")
except ImportError as e:
    SB3_AVAILABLE = False
    print(f"âŒ Stable Baselines3 æœªå®‰è£…: {e}")
    print("è¯·è¿è¡Œ: pip install stable-baselines3")

from ai_game_env import AIGameEnvironment

class AIDecisionTrainer:
    """AIå†³ç­–è®­ç»ƒå™¨"""
    
    def __init__(self, algorithm="PPO", total_timesteps=1000000):
        self.algorithm = algorithm
        self.total_timesteps = total_timesteps
        self.model = None
        self.env = None
        
        # è®­ç»ƒå‚æ•°
        self.training_params = {
            'PPO': {
                'learning_rate': 3e-4,
                'n_steps': 2048,
                'batch_size': 64,
                'n_epochs': 10,
                'gamma': 0.99,
                'gae_lambda': 0.95,
                'clip_range': 0.2,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'max_grad_norm': 0.5
            },
            'DQN': {
                'learning_rate': 1e-4,
                'buffer_size': 100000,
                'learning_starts': 1000,
                'batch_size': 32,
                'gamma': 0.99,
                'train_freq': 4,
                'gradient_steps': 1,
                'target_update_interval': 1000,
                'exploration_fraction': 0.1,
                'exploration_initial_eps': 1.0,
                'exploration_final_eps': 0.05
            },
            'A2C': {
                'learning_rate': 7e-4,
                'n_steps': 5,
                'gamma': 0.99,
                'gae_lambda': 1.0,
                'ent_coef': 0.01,
                'vf_coef': 0.5,
                'max_grad_norm': 0.5
            }
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = f"models/ai_decision_{algorithm.lower()}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        print(f"ğŸ¤– AIå†³ç­–è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç®—æ³•: {algorithm}")
        print(f"   è®­ç»ƒæ­¥æ•°: {total_timesteps:,}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def create_environment(self, num_envs=4):
        """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
        print(f"ğŸŒ åˆ›å»º{num_envs}ä¸ªè®­ç»ƒç¯å¢ƒ...")
        
        def make_env():
            env = AIGameEnvironment()
            env = Monitor(env)
            return env
        
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        self.env = DummyVecEnv([make_env for _ in range(num_envs)])
        
        # æ ‡å‡†åŒ–è§‚å¯Ÿ
        self.env = VecNormalize(
            self.env, 
            norm_obs=True, 
            norm_reward=True,
            clip_obs=10.0,
            clip_reward=10.0
        )
        
        print("âœ… è®­ç»ƒç¯å¢ƒåˆ›å»ºå®Œæˆ")
        return self.env
    
    def create_model(self):
        """åˆ›å»ºAIæ¨¡å‹"""
        if not SB3_AVAILABLE:
            raise ImportError("Stable Baselines3 æœªå®‰è£…")
        
        print(f"ğŸ§  åˆ›å»º{self.algorithm}æ¨¡å‹...")
        
        # è·å–ç®—æ³•å‚æ•°
        params = self.training_params[self.algorithm]
        
        if self.algorithm == "PPO":
            self.model = PPO(
                "MlpPolicy",
                self.env,
                verbose=1,
                tensorboard_log=f"logs/ai_decision_{self.algorithm.lower()}",
                **params
            )
        elif self.algorithm == "DQN":
            self.model = DQN(
                "MlpPolicy",
                self.env,
                verbose=1,
                tensorboard_log=f"logs/ai_decision_{self.algorithm.lower()}",
                **params
            )
        elif self.algorithm == "A2C":
            self.model = A2C(
                "MlpPolicy",
                self.env,
                verbose=1,
                tensorboard_log=f"logs/ai_decision_{self.algorithm.lower()}",
                **params
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")
        
        print(f"âœ… {self.algorithm}æ¨¡å‹åˆ›å»ºå®Œæˆ")
        return self.model
    
    def setup_callbacks(self):
        """è®¾ç½®è®­ç»ƒå›è°ƒ"""
        print("ğŸ“‹ è®¾ç½®è®­ç»ƒå›è°ƒ...")
        
        callbacks = []
        
        # è¯„ä¼°å›è°ƒ
        eval_env = DummyVecEnv([lambda: AIGameEnvironment()])
        eval_env = VecNormalize(eval_env, norm_obs=True, norm_reward=False)
        
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=f"{self.output_dir}/best_model",
            log_path=f"{self.output_dir}/logs",
            eval_freq=max(10000 // 4, 1),  # æ¯10kæ­¥è¯„ä¼°ä¸€æ¬¡
            n_eval_episodes=10,
            deterministic=True,
            render=False
        )
        callbacks.append(eval_callback)
        
        # æ£€æŸ¥ç‚¹å›è°ƒ
        checkpoint_callback = CheckpointCallback(
            save_freq=max(50000 // 4, 1),  # æ¯50kæ­¥ä¿å­˜ä¸€æ¬¡
            save_path=f"{self.output_dir}/checkpoints",
            name_prefix=f"ai_decision_{self.algorithm.lower()}"
        )
        callbacks.append(checkpoint_callback)
        
        print("âœ… è®­ç»ƒå›è°ƒè®¾ç½®å®Œæˆ")
        return callbacks
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        if not self.model:
            raise ValueError("æ¨¡å‹æœªåˆ›å»ºï¼Œè¯·å…ˆè°ƒç”¨create_model()")
        
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ{self.algorithm}æ¨¡å‹...")
        print(f"   æ€»è®­ç»ƒæ­¥æ•°: {self.total_timesteps:,}")
        print(f"   å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # è®¾ç½®å›è°ƒ
        callbacks = self.setup_callbacks()
        
        # å¼€å§‹è®­ç»ƒ
        start_time = time.time()
        
        try:
            self.model.learn(
                total_timesteps=self.total_timesteps,
                callback=callbacks,
                progress_bar=True
            )
            
            training_time = time.time() - start_time
            print(f"âœ… è®­ç»ƒå®Œæˆï¼")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
            print(f"   å¹³å‡é€Ÿåº¦: {self.total_timesteps / training_time:.0f} æ­¥/ç§’")
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            raise
        
        return self.model
    
    def save_model(self, model_name="final"):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not self.model:
            raise ValueError("æ¨¡å‹æœªåˆ›å»º")
        
        model_path = f"{self.output_dir}/{model_name}"
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {model_path}")
        
        try:
            self.model.save(model_path)
            print(f"âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_path}")
            
            # ä¿å­˜ç¯å¢ƒæ ‡å‡†åŒ–å‚æ•°
            env_path = f"{self.output_dir}/env_normalize"
            self.env.save(env_path)
            print(f"âœ… ç¯å¢ƒæ ‡å‡†åŒ–å‚æ•°ä¿å­˜æˆåŠŸ: {env_path}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise
        
        return model_path
    
    def evaluate_model(self, num_episodes=100):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        if not self.model:
            raise ValueError("æ¨¡å‹æœªåˆ›å»º")
        
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½ ({num_episodes} å±€)...")
        
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        eval_env = AIGameEnvironment()
        
        # æ€§èƒ½ç»Ÿè®¡
        episode_rewards = []
        episode_lengths = []
        survival_times = []
        enemies_killed = []
        power_ups_collected = []
        damage_taken = []
        accuracy_rates = []
        
        for episode in range(num_episodes):
            obs, _ = eval_env.reset()
            episode_reward = 0
            step_count = 0
            
            while not eval_env._is_done():
                # ä½¿ç”¨æ¨¡å‹é¢„æµ‹åŠ¨ä½œ
                action, _ = self.model.predict(obs, deterministic=True)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                obs, reward, done, truncated, info = eval_env.step(action)
                episode_reward += reward
                step_count += 1
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            episode_rewards.append(episode_reward)
            episode_lengths.append(step_count)
            survival_times.append(eval_env.stats['survival_time'])
            enemies_killed.append(eval_env.stats['enemies_killed'])
            power_ups_collected.append(eval_env.stats['power_ups_collected'])
            damage_taken.append(eval_env.stats['damage_taken'])
            
            accuracy = eval_env.stats['shots_hit'] / max(1, eval_env.stats['shots_fired'])
            accuracy_rates.append(accuracy)
            
            if (episode + 1) % 10 == 0:
                print(f"   å®Œæˆ {episode + 1}/{num_episodes} å±€")
        
        # è®¡ç®—ç»Ÿè®¡ç»“æœ
        results = {
            'episode_rewards': {
                'mean': np.mean(episode_rewards),
                'std': np.std(episode_rewards),
                'min': np.min(episode_rewards),
                'max': np.max(episode_rewards)
            },
            'episode_lengths': {
                'mean': np.mean(episode_lengths),
                'std': np.std(episode_lengths)
            },
            'survival_times': {
                'mean': np.mean(survival_times),
                'std': np.std(survival_times)
            },
            'enemies_killed': {
                'mean': np.mean(enemies_killed),
                'std': np.std(enemies_killed)
            },
            'power_ups_collected': {
                'mean': np.mean(power_ups_collected),
                'std': np.std(power_ups_collected)
            },
            'damage_taken': {
                'mean': np.mean(damage_taken),
                'std': np.std(damage_taken)
            },
            'accuracy_rates': {
                'mean': np.mean(accuracy_rates),
                'std': np.std(accuracy_rates)
            }
        }
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        print(f"   å¹³å‡å¥–åŠ±: {results['episode_rewards']['mean']:.2f} Â± {results['episode_rewards']['std']:.2f}")
        print(f"   å¹³å‡ç”Ÿå­˜æ—¶é—´: {results['survival_times']['mean']:.1f} Â± {results['survival_times']['std']:.1f} æ­¥")
        print(f"   å¹³å‡å‡»æ€æ•°: {results['enemies_killed']['mean']:.1f} Â± {results['enemies_killed']['std']:.1f}")
        print(f"   å¹³å‡é“å…·æ”¶é›†: {results['power_ups_collected']['mean']:.1f} Â± {results['power_ups_collected']['std']:.1f}")
        print(f"   å¹³å‡ä¼¤å®³: {results['damage_taken']['mean']:.1f} Â± {results['damage_taken']['std']:.1f}")
        print(f"   å¹³å‡å‘½ä¸­ç‡: {results['accuracy_rates']['mean']:.3f} Â± {results['accuracy_rates']['std']:.3f}")
        
        return results
    
    def test_model(self, num_episodes=5):
        """æµ‹è¯•æ¨¡å‹ï¼ˆå¯è§†åŒ–ï¼‰"""
        if not self.model:
            raise ValueError("æ¨¡å‹æœªåˆ›å»º")
        
        print(f"ğŸ® æµ‹è¯•æ¨¡å‹ ({num_episodes} å±€)...")
        
        for episode in range(num_episodes):
            print(f"\n--- ç¬¬ {episode + 1} å±€æµ‹è¯• ---")
            
            obs, _ = self.env.reset()
            episode_reward = 0
            step_count = 0
            
            while not self.env._is_done():
                # ä½¿ç”¨æ¨¡å‹é¢„æµ‹åŠ¨ä½œ
                action, _ = self.model.predict(obs, deterministic=True)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                obs, reward, done, truncated, info = self.env.step(action)
                episode_reward += reward
                step_count += 1
                
                # æ¯100æ­¥æ‰“å°ä¸€æ¬¡çŠ¶æ€
                if step_count % 100 == 0:
                    print(f"   æ­¥æ•°: {step_count}, å¥–åŠ±: {episode_reward:.2f}, ç”Ÿå‘½: {self.env.life}")
            
            print(f"   ç¬¬ {episode + 1} å±€å®Œæˆ:")
            print(f"     æ€»æ­¥æ•°: {step_count}")
            print(f"     æ€»å¥–åŠ±: {episode_reward:.2f}")
            print(f"     æœ€ç»ˆç”Ÿå‘½: {self.env.life}")
            print(f"     å‡»æ€æ•°: {self.env.stats['enemies_killed']}")
            print(f"     é“å…·æ”¶é›†: {self.env.stats['power_ups_collected']}")
            print(f"     å‘½ä¸­ç‡: {self.env.stats['shots_hit'] / max(1, self.env.stats['shots_fired']):.3f}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AIå†³ç­–ç³»ç»Ÿè®­ç»ƒ")
    parser.add_argument("--algorithm", type=str, default="PPO", 
                       choices=["PPO", "DQN", "A2C"], help="è®­ç»ƒç®—æ³•")
    parser.add_argument("--timesteps", type=int, default=1000000, 
                       help="è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--envs", type=int, default=4, 
                       help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
    parser.add_argument("--eval", action="store_true", 
                       help="è®­ç»ƒåè¯„ä¼°æ¨¡å‹")
    parser.add_argument("--test", action="store_true", 
                       help="è®­ç»ƒåæµ‹è¯•æ¨¡å‹")
    
    args = parser.parse_args()
    
    if not SB3_AVAILABLE:
        print("âŒ æ— æ³•ç»§ç»­ï¼šStable Baselines3 æœªå®‰è£…")
        print("è¯·è¿è¡Œ: pip install stable-baselines3")
        return
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = AIDecisionTrainer(
            algorithm=args.algorithm,
            total_timesteps=args.timesteps
        )
        
        # åˆ›å»ºç¯å¢ƒ
        trainer.create_environment(num_envs=args.envs)
        
        # åˆ›å»ºæ¨¡å‹
        trainer.create_model()
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        model_path = trainer.save_model()
        
        # è¯„ä¼°æ¨¡å‹
        if args.eval:
            trainer.evaluate_model()
        
        # æµ‹è¯•æ¨¡å‹
        if args.test:
            trainer.test_model()
        
        print(f"\nğŸ‰ AIå†³ç­–è®­ç»ƒå®Œæˆï¼")
        print(f"   æ¨¡å‹ä¿å­˜ä½ç½®: {model_path}")
        print(f"   å¯ä»¥åœ¨æ¸¸æˆä¸­ä½¿ç”¨è¿™ä¸ªæ¨¡å‹äº†ï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
