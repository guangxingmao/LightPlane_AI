#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆAIæˆ˜æœºè®­ç»ƒè„šæœ¬
è®­ç»ƒAIæˆ˜æœºä»¥é€‚åº”åŠ¨æ€æ¸¸æˆç­–ç•¥ï¼Œå®ç°çœŸæ­£çš„ä¸‰é‡AIååŒ
"""

import os
import sys
import argparse
import time
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from stable_baselines3 import PPO, DQN, A2C
    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
    from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
    from stable_baselines3.common.utils import set_random_seed
    SB3_AVAILABLE = True
    print("âœ… Stable Baselines3 å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ Stable Baselines3 å¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·å®‰è£…: pip install stable-baselines3")
    SB3_AVAILABLE = False

try:
    from integrated_plane_env import IntegratedPlaneFighterEnv
    print("âœ… é›†æˆè®­ç»ƒç¯å¢ƒå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ é›†æˆè®­ç»ƒç¯å¢ƒå¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ’¡ æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå¯¼å…¥")
    sys.exit(1)

class IntegratedPlaneTrainer:
    """é›†æˆAIæˆ˜æœºè®­ç»ƒå™¨"""
    
    def __init__(self, algorithm="PPO", total_timesteps=2000000):
        self.algorithm = algorithm
        self.total_timesteps = total_timesteps
        
        # è®­ç»ƒå‚æ•°
        self.training_params = {
            "PPO": {
                "learning_rate": 3e-4,
                "n_steps": 2048,
                "batch_size": 64,
                "n_epochs": 10,
                "gamma": 0.99,
                "gae_lambda": 0.95,
                "clip_range": 0.2,
                "clip_range_vf": None,
                "normalize_advantage": True,
                "ent_coef": 0.01,
                "vf_coef": 0.5,
                "max_grad_norm": 0.5,
                "use_sde": False,  # ç¦»æ•£åŠ¨ä½œç©ºé—´ä¸æ”¯æŒgSDE
                "sde_sample_freq": -1,
                "target_kl": None,
                "tensorboard_log": None,
                "policy_kwargs": {
                    "net_arch": [dict(pi=[256, 256], vf=[256, 256])]  # æ›´å¤§çš„ç½‘ç»œ
                }
            },
            "DQN": {
                "learning_rate": 1e-4,
                "buffer_size": 1000000,
                "learning_starts": 100000,
                "batch_size": 32,
                "tau": 1.0,
                "gamma": 0.99,
                "train_freq": 4,
                "gradient_steps": 1,
                "target_update_interval": 10000,
                "exploration_fraction": 0.1,
                "exploration_initial_eps": 1.0,
                "exploration_final_eps": 0.05,
                "max_grad_norm": 10,
                "policy_kwargs": {
                    "net_arch": [256, 256]
                }
            },
            "A2C": {
                "learning_rate": 7e-4,
                "n_steps": 5,
                "gamma": 0.99,
                "gae_lambda": 1.0,
                "ent_coef": 0.01,
                "vf_coef": 0.25,
                "max_grad_norm": 0.5,
                "rms_prop_eps": 1e-5,
                "use_rms_prop": True,
                "use_sde": False,
                "sde_sample_freq": -1,
                "policy_kwargs": {
                    "net_arch": [dict(pi=[256, 256], vf=[256, 256])]
                }
            }
        }
        
        # è¾“å‡ºç›®å½•
        self.output_dir = f"models/integrated_plane_{algorithm.lower()}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾ç½®éšæœºç§å­
        set_random_seed(42)
        
        print(f"ğŸ¯ é›†æˆAIæˆ˜æœºè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç®—æ³•: {algorithm}")
        print(f"   è®­ç»ƒæ­¥æ•°: {total_timesteps:,}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"   ç¯å¢ƒç±»å‹: é›†æˆè®­ç»ƒç¯å¢ƒï¼ˆæ”¯æŒåŠ¨æ€ç­–ç•¥ï¼‰")
    
    def create_environment(self, num_envs=8):
        """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
        print(f"ğŸŒ åˆ›å»º {num_envs} ä¸ªé›†æˆè®­ç»ƒç¯å¢ƒ...")
        
        def make_env():
            env = IntegratedPlaneFighterEnv()
            return env
        
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        env = DummyVecEnv([make_env for _ in range(num_envs)])
        
        # æ·»åŠ ç¯å¢ƒæ ‡å‡†åŒ–
        env = VecNormalize(
            env,
            norm_obs=True,
            norm_reward=True,
            clip_obs=10.0,
            clip_reward=10.0
        )
        
        print(f"âœ… é›†æˆè®­ç»ƒç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        print(f"   ç¯å¢ƒæ•°é‡: {num_envs}")
        print(f"   è§‚å¯Ÿç©ºé—´: {env.observation_space}")
        print(f"   åŠ¨ä½œç©ºé—´: {env.action_space}")
        print(f"   ç¯å¢ƒæ ‡å‡†åŒ–: å·²å¯ç”¨")
        
        return env
    
    def create_eval_environment(self):
        """åˆ›å»ºè¯„ä¼°ç¯å¢ƒ"""
        print(f"ğŸŒ åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
        
        def make_eval_env():
            env = IntegratedPlaneFighterEnv(render_mode=None)
            return env
        
        eval_env = DummyVecEnv([make_eval_env])
        
        # ä½¿ç”¨è®­ç»ƒç¯å¢ƒçš„æ ‡å‡†åŒ–å‚æ•°
        eval_env = VecNormalize(
            eval_env,
            norm_obs=True,
            norm_reward=False,  # è¯„ä¼°æ—¶ä¸æ ‡å‡†åŒ–å¥–åŠ±
            clip_obs=10.0
        )
        
        print(f"âœ… è¯„ä¼°ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        return eval_env
    
    def create_model(self, env):
        """åˆ›å»ºAIæ¨¡å‹"""
        print(f"ğŸ§  åˆ›å»º {self.algorithm} æ¨¡å‹...")
        
        if self.algorithm == "PPO":
            # ç§»é™¤é‡å¤çš„tensorboard_logå‚æ•°
            ppo_params = self.training_params["PPO"].copy()
            if 'tensorboard_log' in ppo_params:
                del ppo_params['tensorboard_log']
            
            model = PPO(
                "MlpPolicy", 
                env, 
                verbose=1,
                tensorboard_log=f"{self.output_dir}/tensorboard",
                **ppo_params
            )
            
        elif self.algorithm == "DQN":
            model = DQN(
                "MlpPolicy", 
                env, 
                verbose=1,
                tensorboard_log=f"{self.output_dir}/tensorboard",
                **self.training_params["DQN"]
            )
            
        elif self.algorithm == "A2C":
            model = A2C(
                "MlpPolicy", 
                env, 
                verbose=1,
                tensorboard_log=f"{self.output_dir}/tensorboard",
                **self.training_params["A2C"]
            )
        
        print(f"âœ… {self.algorithm} æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"   ç­–ç•¥ç½‘ç»œ: MLPç­–ç•¥")
        print(f"   ç½‘ç»œæ¶æ„: {self.training_params[self.algorithm]['policy_kwargs']}")
        
        return model
    
    def setup_callbacks(self, eval_env):
        """è®¾ç½®è®­ç»ƒå›è°ƒ"""
        print(f"ğŸ”§ è®¾ç½®è®­ç»ƒå›è°ƒ...")
        
        # è¯„ä¼°å›è°ƒ
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=f"{self.output_dir}/best_model",
            log_path=f"{self.output_dir}/eval_logs",
            eval_freq=max(10000 // 8, 1),  # æ¯10000æ­¥è¯„ä¼°ä¸€æ¬¡
            deterministic=True,
            render=False
        )
        
        # æ£€æŸ¥ç‚¹å›è°ƒ
        checkpoint_callback = CheckpointCallback(
            save_freq=max(50000 // 8, 1),  # æ¯50000æ­¥ä¿å­˜ä¸€æ¬¡
            save_path=f"{self.output_dir}/checkpoints",
            name_prefix="integrated_plane_model"
        )
        
        callbacks = [eval_callback, checkpoint_callback]
        
        print(f"âœ… è®­ç»ƒå›è°ƒè®¾ç½®å®Œæˆ")
        print(f"   è¯„ä¼°é¢‘ç‡: æ¯10000æ­¥")
        print(f"   æ£€æŸ¥ç‚¹é¢‘ç‡: æ¯50000æ­¥")
        print(f"   æœ€ä½³æ¨¡å‹ä¿å­˜: å·²å¯ç”¨")
        
        return callbacks
    
    def train(self, model, env, eval_env):
        """å¼€å§‹è®­ç»ƒ"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒé›†æˆAIæˆ˜æœº...")
        print(f"   ç®—æ³•: {self.algorithm}")
        print(f"   æ€»æ­¥æ•°: {self.total_timesteps:,}")
        print(f"   ç¯å¢ƒæ•°é‡: {env.num_envs}")
        print(f"   è®­ç»ƒç¯å¢ƒ: é›†æˆè®­ç»ƒç¯å¢ƒï¼ˆåŠ¨æ€ç­–ç•¥ï¼‰")
        
        start_time = time.time()
        
        # è®­ç»ƒæ¨¡å‹
        model.learn(
            total_timesteps=self.total_timesteps,
            callback=self.setup_callbacks(eval_env),
            progress_bar=True
        )
        
        training_time = time.time() - start_time
        
        print(f"âœ… è®­ç»ƒå®Œæˆ!")
        print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
        print(f"   å¹³å‡é€Ÿåº¦: {self.total_timesteps / training_time:.0f} æ­¥/ç§’")
        
        return model
    
    def save_model(self, model, env, model_name="final"):
        """ä¿å­˜æ¨¡å‹"""
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"{self.output_dir}/{model_name}"
        model.save(model_path)
        print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        
        # ä¿å­˜ç¯å¢ƒæ ‡å‡†åŒ–å‚æ•°
        env_normalize_path = f"{self.output_dir}/env_normalize"
        env.save(env_normalize_path)
        print(f"âœ… ç¯å¢ƒæ ‡å‡†åŒ–å‚æ•°ä¿å­˜åˆ°: {env_normalize_path}")
        
        return model_path, env_normalize_path
    
    def evaluate_model(self, model, eval_env, num_episodes=100):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"ğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        print(f"   è¯„ä¼°å›åˆæ•°: {num_episodes}")
        
        # é‡ç½®ç¯å¢ƒæ ‡å‡†åŒ–å‚æ•°
        eval_env.norm_reward = False
        
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(num_episodes):
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0
            
            while True:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, _ = eval_env.step(action)
                episode_reward += reward[0]
                episode_length += 1
                
                if terminated[0] or truncated[0]:
                    break
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if (episode + 1) % 10 == 0:
                print(f"   è¯„ä¼°è¿›åº¦: {episode + 1}/{num_episodes}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_reward = np.mean(episode_rewards)
        std_reward = np.std(episode_rewards)
        mean_length = np.mean(episode_lengths)
        max_reward = np.max(episode_rewards)
        min_reward = np.min(episode_rewards)
        
        print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
        print(f"   å¹³å‡é•¿åº¦: {mean_length:.1f} æ­¥")
        print(f"   æœ€é«˜å¥–åŠ±: {max_reward:.2f}")
        print(f"   æœ€ä½å¥–åŠ±: {min_reward:.2f}")
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_results = {
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'mean_length': mean_length,
            'max_reward': max_reward,
            'min_reward': min_reward
        }
        
        eval_path = f"{self.output_dir}/evaluation_results.npz"
        np.savez(eval_path, **eval_results)
        print(f"âœ… è¯„ä¼°ç»“æœä¿å­˜åˆ°: {eval_path}")
        
        return eval_results
    
    def test_model(self, model, test_env, num_episodes=5):
        """æµ‹è¯•æ¨¡å‹ï¼ˆå¯è§†åŒ–ï¼‰"""
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹...")
        print(f"   æµ‹è¯•å›åˆæ•°: {num_episodes}")
        print(f"   æ³¨æ„: è¿™æ˜¯å¯è§†åŒ–æµ‹è¯•ï¼Œä¼šæ˜¾ç¤ºæ¸¸æˆçª—å£")
        
        # åˆ›å»ºå¯è§†åŒ–æµ‹è¯•ç¯å¢ƒ
        test_env = IntegratedPlaneFighterEnv(render_mode="human")
        
        for episode in range(num_episodes):
            print(f"   æµ‹è¯•å›åˆ {episode + 1}/{num_episodes}")
            obs, _ = test_env.reset()
            episode_reward = 0
            episode_length = 0
            
            while True:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = test_env.step(action)
                episode_reward += reward
                episode_length += 1
                
                # æ˜¾ç¤ºä¿¡æ¯
                if episode_length % 100 == 0:
                    print(f"     æ­¥æ•°: {episode_length}, å¥–åŠ±: {episode_reward:.2f}, åˆ†æ•°: {info['score']}")
                
                if terminated or truncated:
                    break
            
            print(f"   å›åˆ {episode + 1} å®Œæˆ: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}")
        
        test_env.close()
        print(f"âœ… æ¨¡å‹æµ‹è¯•å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é›†æˆAIæˆ˜æœºè®­ç»ƒ")
    parser.add_argument("--algorithm", type=str, default="PPO", 
                       choices=["PPO", "DQN", "A2C"], help="è®­ç»ƒç®—æ³•")
    parser.add_argument("--timesteps", type=int, default=2000000, 
                       help="è®­ç»ƒæ€»æ­¥æ•°")
    parser.add_argument("--envs", type=int, default=8, 
                       help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
    parser.add_argument("--eval", action="store_true", 
                       help="è®­ç»ƒåè¯„ä¼°æ¨¡å‹")
    parser.add_argument("--test", action="store_true", 
                       help="è®­ç»ƒåæµ‹è¯•æ¨¡å‹")
    
    args = parser.parse_args()
    
    print("ğŸ¯ é›†æˆAIæˆ˜æœºè®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    print(f"ç®—æ³•: {args.algorithm}")
    print(f"è®­ç»ƒæ­¥æ•°: {args.timesteps:,}")
    print(f"å¹¶è¡Œç¯å¢ƒ: {args.envs}")
    print(f"è®­ç»ƒåè¯„ä¼°: {args.eval}")
    print(f"è®­ç»ƒåæµ‹è¯•: {args.test}")
    print("=" * 50)
    
    if not SB3_AVAILABLE:
        print("âŒ æ— æ³•è®­ç»ƒï¼ŒStable Baselines3 æœªå®‰è£…")
        return
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = IntegratedPlaneTrainer(args.algorithm, args.timesteps)
        
        # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        train_env = trainer.create_environment(args.envs)
        
        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        eval_env = trainer.create_eval_environment()
        
        # åˆ›å»ºæ¨¡å‹
        model = trainer.create_model(train_env)
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train(model, train_env, eval_env)
        
        # ä¿å­˜æ¨¡å‹
        model_path, env_normalize_path = trainer.save_model(model, train_env)
        
        # è¯„ä¼°æ¨¡å‹
        if args.eval:
            trainer.evaluate_model(model, eval_env)
        
        # æµ‹è¯•æ¨¡å‹
        if args.test:
            trainer.test_model(model, None)
        
        print(f"\nğŸ‰ é›†æˆAIæˆ˜æœºè®­ç»ƒå®Œæˆ!")
        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"ç¯å¢ƒæ ‡å‡†åŒ–: {env_normalize_path}")
        print(f"ä¸‹ä¸€æ­¥: å°†æ–°æ¨¡å‹é›†æˆåˆ°æ¸¸æˆä¸­ï¼Œå®ç°çœŸæ­£çš„ä¸‰é‡AIååŒ")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

