"""
ä½¿ç”¨Stable Baselines3è®­ç»ƒé£æœºå¤§æˆ˜AI
ä½¿ç”¨PPOç®—æ³•è¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒ
"""

import os
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
import numpy as np

# å¯¼å…¥æˆ‘ä»¬çš„ç¯å¢ƒ
from plane_fighter_env import PlaneFighterEnv


def create_training_env():
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
    def _init():
        return PlaneFighterEnv(
            screen_width=1280,
            screen_height=720,
            render_mode=None  # è®­ç»ƒæ—¶ä¸æ¸²æŸ“ï¼Œæé«˜é€Ÿåº¦
        )
    return _init


def create_eval_env():
    """åˆ›å»ºè¯„ä¼°ç¯å¢ƒ"""
    def _init():
        return PlaneFighterEnv(
            screen_width=1280,
            screen_height=720,
            render_mode=None
        )
    return _init


def train_ppo_agent(
    total_timesteps=500000,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,
    vf_coef=0.5,
    max_grad_norm=0.5,
    save_path="./models/",
    log_path="./logs/",
    continue_from=None
):
    """
    è®­ç»ƒPPOæ™ºèƒ½ä½“
    
    Args:
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        learning_rate: å­¦ä¹ ç‡
        n_steps: æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°
        batch_size: æ‰¹å¤§å°
        n_epochs: æ¯æ¬¡æ›´æ–°çš„è½®æ•°
        gamma: æŠ˜æ‰£å› å­
        gae_lambda: GAEå‚æ•°
        clip_range: PPOè£å‰ªèŒƒå›´
        ent_coef: ç†µç³»æ•°
        vf_coef: ä»·å€¼å‡½æ•°ç³»æ•°
        max_grad_norm: æ¢¯åº¦è£å‰ª
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        log_path: æ—¥å¿—è·¯å¾„
    """
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_path, exist_ok=True)
    os.makedirs(log_path, exist_ok=True)
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒé£æœºå¤§æˆ˜AI...")
    print(f"ğŸ“Š æ€»è®­ç»ƒæ­¥æ•°: {total_timesteps:,}")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜è·¯å¾„: {save_path}")
    print(f"ğŸ“ æ—¥å¿—è·¯å¾„: {log_path}")
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒï¼ˆå¹¶è¡Œè®­ç»ƒï¼‰
    n_envs = 4  # 4ä¸ªå¹¶è¡Œç¯å¢ƒ
    env = make_vec_env(create_training_env(), n_envs=n_envs)
    eval_env = make_vec_env(create_eval_env(), n_envs=1)
    
    print(f"ğŸ”„ åˆ›å»ºäº† {n_envs} ä¸ªå¹¶è¡Œè®­ç»ƒç¯å¢ƒ")
    
    # åˆ›å»ºæˆ–åŠ è½½PPOæ¨¡å‹
    if continue_from and os.path.exists(f"{continue_from}.zip"):
        print(f"ğŸ”„ ä»ç°æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ: {continue_from}")
        model = PPO.load(continue_from, env=env, verbose=1)
        print("âœ… ç°æœ‰æ¨¡å‹åŠ è½½æˆåŠŸï¼Œç»§ç»­è®­ç»ƒ...")
    else:
        print("ğŸ†• åˆ›å»ºæ–°çš„PPOæ¨¡å‹")
        model = PPO(
            "MlpPolicy",  # å¤šå±‚æ„ŸçŸ¥æœºç­–ç•¥
            env,
            learning_rate=learning_rate,
            n_steps=n_steps,
            batch_size=batch_size,
            n_epochs=n_epochs,
            gamma=gamma,
            gae_lambda=gae_lambda,
            clip_range=clip_range,
            ent_coef=ent_coef,
            vf_coef=vf_coef,
            max_grad_norm=max_grad_norm,
            verbose=1,
            device="auto",  # è‡ªåŠ¨é€‰æ‹©CPU/GPU
            tensorboard_log=log_path
        )
    
    print("ğŸ§  PPOæ¨¡å‹åˆ›å»ºå®Œæˆ!")
    print(f"ğŸ“ è§‚å¯Ÿç©ºé—´: {env.observation_space}")
    print(f"ğŸ® åŠ¨ä½œç©ºé—´: {env.action_space}")
    
    # è®¾ç½®å›è°ƒå‡½æ•°
    callbacks = []
    
    # è¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=save_path,
        log_path=log_path,
        eval_freq=10000,  # æ¯10000æ­¥è¯„ä¼°ä¸€æ¬¡
        deterministic=True,
        render=False,
        verbose=1
    )
    callbacks.append(eval_callback)
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,  # æ¯50000æ­¥ä¿å­˜ä¸€æ¬¡
        save_path=save_path,
        name_prefix="ppo_plane_fighter"
    )
    callbacks.append(checkpoint_callback)
    
    print("ğŸ“‹ å›è°ƒå‡½æ•°è®¾ç½®å®Œæˆ")
    
    try:
        # å¼€å§‹è®­ç»ƒ
        print("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
        model.learn(
            total_timesteps=total_timesteps,
            callback=callbacks,
            progress_bar=True
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(save_path, "ppo_plane_fighter_final")
        model.save(final_model_path)
        print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        
    except KeyboardInterrupt:
        print("â¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        # ä¿å­˜å½“å‰æ¨¡å‹
        interrupted_model_path = os.path.join(save_path, "ppo_plane_fighter_interrupted")
        model.save(interrupted_model_path)
        print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupted_model_path}")
    
    finally:
        # å…³é—­ç¯å¢ƒ
        env.close()
        eval_env.close()
        print("âœ… è®­ç»ƒå®Œæˆï¼Œç¯å¢ƒå·²å…³é—­")


def test_trained_model(model_path, episodes=5):
    """
    æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        episodes: æµ‹è¯•å›åˆæ•°
    """
    
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
    env = PlaneFighterEnv(render_mode="rgb_array")
    
    try:
        # åŠ è½½æ¨¡å‹
        model = PPO.load(model_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•å¤šä¸ªå›åˆ
        total_scores = []
        total_steps = []
        
        for episode in range(episodes):
            obs, info = env.reset()
            episode_score = 0
            episode_steps = 0
            
            print(f"ğŸ® å¼€å§‹ç¬¬ {episode + 1} å›åˆ...")
            
            while True:
                # AIåšå†³ç­–
                action, _states = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = env.step(action)
                
                episode_score += reward
                episode_steps += 1
                
                if terminated or truncated:
                    break
            
            final_score = info['score']
            total_scores.append(final_score)
            total_steps.append(episode_steps)
            
            print(f"ğŸ“Š ç¬¬ {episode + 1} å›åˆç»“æŸ:")
            print(f"   æ¸¸æˆå¾—åˆ†: {final_score}")
            print(f"   å­˜æ´»æ­¥æ•°: {episode_steps}")
            print(f"   æ€»å¥–åŠ±: {episode_score:.2f}")
            print()
        
        # ç»Ÿè®¡ç»“æœ
        avg_score = np.mean(total_scores)
        avg_steps = np.mean(total_steps)
        
        print("ğŸ“ˆ æµ‹è¯•ç»“æœç»Ÿè®¡:")
        print(f"   å¹³å‡æ¸¸æˆå¾—åˆ†: {avg_score:.2f}")
        print(f"   å¹³å‡å­˜æ´»æ­¥æ•°: {avg_steps:.0f}")
        print(f"   æœ€é«˜å¾—åˆ†: {max(total_scores)}")
        print(f"   æœ€é•¿å­˜æ´»: {max(total_steps)}")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    
    finally:
        env.close()


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é£æœºå¤§æˆ˜AIè®­ç»ƒ")
    parser.add_argument("--mode", choices=["train", "test"], default="train",
                       help="è¿è¡Œæ¨¡å¼: train(è®­ç»ƒ) æˆ– test(æµ‹è¯•)")
    parser.add_argument("--timesteps", type=int, default=500000,
                       help="è®­ç»ƒæ­¥æ•° (é»˜è®¤: 500000)")
    parser.add_argument("--model", type=str, default="./models/best_model",
                       help="æ¨¡å‹è·¯å¾„ (æµ‹è¯•æ¨¡å¼ä½¿ç”¨)")
    parser.add_argument("--episodes", type=int, default=5,
                       help="æµ‹è¯•å›åˆæ•° (é»˜è®¤: 5)")
    parser.add_argument("--continue_from", type=str, default=None,
                       help="ä»ç°æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒ (æ¨¡å‹è·¯å¾„ï¼Œä¸åŒ…å«.zipæ‰©å±•å)")
    
    args = parser.parse_args()
    
    if args.mode == "train":
        print("ğŸ¯ è®­ç»ƒæ¨¡å¼")
        train_ppo_agent(total_timesteps=args.timesteps, continue_from=args.continue_from)
    
    elif args.mode == "test":
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼")
        test_trained_model(args.model, args.episodes)


if __name__ == "__main__":
    main()
