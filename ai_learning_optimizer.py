#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå­¦ä¹ å’Œä¼˜åŒ–ç³»ç»Ÿ - çœŸæ­£çš„å­¦ä¹ å’Œä¼˜åŒ–èƒ½åŠ›
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
import time
import json
import os
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt

@dataclass
class LearningExperience:
    """å­¦ä¹ ç»éªŒ"""
    state: Dict[str, Any]
    action: Dict[str, Any]
    reward: float
    next_state: Dict[str, Any]
    outcome: Dict[str, Any]
    timestamp: float
    session_id: str

@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    parameter_name: str
    old_value: Any
    new_value: Any
    improvement: float
    confidence: float
    reasoning: str

class MetaLearningNetwork(nn.Module):
    """å…ƒå­¦ä¹ ç½‘ç»œ - å­¦ä¹ å¦‚ä½•å­¦ä¹ """
    
    def __init__(self, input_size=100, hidden_size=256, output_size=50):
        super(MetaLearningNetwork, self).__init__()
        self.meta_learner = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, output_size),
            nn.Tanh()
        )
        
        # å­¦ä¹ ç‡é¢„æµ‹å™¨
        self.lr_predictor = nn.Sequential(
            nn.Linear(output_size, hidden_size // 4),
            nn.ReLU(),
            nn.Linear(hidden_size // 4, 1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        meta_features = self.meta_learner(x)
        learning_rate = self.lr_predictor(meta_features)
        return meta_features, learning_rate

class AdaptiveOptimizer:
    """è‡ªé€‚åº”ä¼˜åŒ–å™¨ - æ ¹æ®å­¦ä¹ æ•ˆæœè°ƒæ•´ä¼˜åŒ–ç­–ç•¥"""
    
    def __init__(self):
        self.optimization_history = deque(maxlen=1000)
        self.performance_metrics = {
            'learning_rate': [],
            'convergence_speed': [],
            'final_performance': [],
            'stability': []
        }
        
        # ä¼˜åŒ–ç­–ç•¥
        self.optimization_strategies = {
            'gradient_descent': self._gradient_descent_optimization,
            'genetic_algorithm': self._genetic_optimization,
            'bayesian_optimization': self._bayesian_optimization,
            'reinforcement_learning': self._rl_optimization
        }
        
        self.current_strategy = 'gradient_descent'
        self.strategy_performance = {k: 0.0 for k in self.optimization_strategies.keys()}
    
    def optimize_parameters(self, model, loss_function, data, **kwargs):
        """ä¼˜åŒ–æ¨¡å‹å‚æ•°"""
        strategy = self.optimization_strategies[self.current_strategy]
        result = strategy(model, loss_function, data, **kwargs)
        
        # è®°å½•ä¼˜åŒ–å†å²
        self.optimization_history.append(result)
        
        # æ›´æ–°ç­–ç•¥æ€§èƒ½
        self._update_strategy_performance(result)
        
        # è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥
        self._adapt_strategy()
        
        return result
    
    def _gradient_descent_optimization(self, model, loss_function, data, **kwargs):
        """æ¢¯åº¦ä¸‹é™ä¼˜åŒ–"""
        optimizer = optim.Adam(model.parameters(), lr=kwargs.get('lr', 0.001))
        
        losses = []
        for epoch in range(kwargs.get('epochs', 100)):
            optimizer.zero_grad()
            loss = loss_function(model, data)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
            
            if epoch % 20 == 0:
                print(f"ğŸ“‰ æ¢¯åº¦ä¸‹é™ä¼˜åŒ– - Epoch {epoch}, Loss: {loss.item():.4f}")
        
        return {
            'strategy': 'gradient_descent',
            'final_loss': losses[-1],
            'convergence_speed': len(losses),
            'stability': np.std(losses[-10:]) if len(losses) >= 10 else np.std(losses)
        }
    
    def _genetic_optimization(self, model, loss_function, data, **kwargs):
        """é—ä¼ ç®—æ³•ä¼˜åŒ–"""
        population_size = kwargs.get('population_size', 50)
        generations = kwargs.get('generations', 100)
        
        # è·å–æ¨¡å‹å‚æ•°
        params = list(model.parameters())
        param_shapes = [p.shape for p in params]
        
        # åˆå§‹åŒ–ç§ç¾¤
        population = []
        for _ in range(population_size):
            individual = []
            for param in params:
                individual.append(torch.randn_like(param))
            population.append(individual)
        
        best_fitness = float('inf')
        best_individual = None
        
        for generation in range(generations):
            # è¯„ä¼°é€‚åº”åº¦
            fitness_scores = []
            for individual in population:
                # ä¸´æ—¶è®¾ç½®å‚æ•°
                for i, param in enumerate(params):
                    param.data = individual[i]
                
                try:
                    loss = loss_function(model, data)
                    fitness_scores.append(loss.item())
                except:
                    fitness_scores.append(float('inf'))
            
            # é€‰æ‹©æœ€ä½³ä¸ªä½“
            best_idx = np.argmin(fitness_scores)
            if fitness_scores[best_idx] < best_fitness:
                best_fitness = fitness_scores[best_idx]
                best_individual = population[best_idx]
            
            # ç”Ÿæˆæ–°ç§ç¾¤
            new_population = []
            for _ in range(population_size):
                parent1 = population[random.randint(0, population_size-1)]
                parent2 = population[random.randint(0, population_size-1)]
                
                # äº¤å‰
                child = []
                for i in range(len(parent1)):
                    if random.random() < 0.5:
                        child.append(parent1[i].clone())
                    else:
                        child.append(parent2[i].clone())
                
                # å˜å¼‚
                for i in range(len(child)):
                    if random.random() < 0.1:
                        child[i] += torch.randn_like(child[i]) * 0.1
                
                new_population.append(child)
            
            population = new_population
            
            if generation % 20 == 0:
                print(f"ğŸ§¬ é—ä¼ ç®—æ³•ä¼˜åŒ– - Generation {generation}, Best Fitness: {best_fitness:.4f}")
        
        # åº”ç”¨æœ€ä½³å‚æ•°
        if best_individual:
            for i, param in enumerate(params):
                param.data = best_individual[i]
        
        return {
            'strategy': 'genetic_algorithm',
            'final_loss': best_fitness,
            'convergence_speed': generations,
            'stability': 0.0  # é—ä¼ ç®—æ³•ç¨³å®šæ€§éš¾ä»¥é‡åŒ–
        }
    
    def _bayesian_optimization(self, model, loss_function, data, **kwargs):
        """è´å¶æ–¯ä¼˜åŒ–"""
        # ç®€åŒ–çš„è´å¶æ–¯ä¼˜åŒ–å®ç°
        n_trials = kwargs.get('n_trials', 50)
        
        # å®šä¹‰å‚æ•°ç©ºé—´
        param_ranges = {
            'lr': (0.0001, 0.01),
            'batch_size': (16, 128),
            'hidden_size': (64, 512)
        }
        
        best_params = None
        best_loss = float('inf')
        
        for trial in range(n_trials):
            # éšæœºé‡‡æ ·å‚æ•°
            params = {
                'lr': random.uniform(*param_ranges['lr']),
                'batch_size': random.randint(*param_ranges['batch_size']),
                'hidden_size': random.randint(*param_ranges['hidden_size'])
            }
            
            # åº”ç”¨å‚æ•°
            optimizer = optim.Adam(model.parameters(), lr=params['lr'])
            
            # è®­ç»ƒå‡ æ­¥
            for _ in range(10):
                optimizer.zero_grad()
                loss = loss_function(model, data)
                loss.backward()
                optimizer.step()
            
            # è¯„ä¼°
            final_loss = loss_function(model, data).item()
            
            if final_loss < best_loss:
                best_loss = final_loss
                best_params = params
            
            if trial % 10 == 0:
                print(f"ğŸ”® è´å¶æ–¯ä¼˜åŒ– - Trial {trial}, Best Loss: {best_loss:.4f}")
        
        return {
            'strategy': 'bayesian_optimization',
            'final_loss': best_loss,
            'convergence_speed': n_trials,
            'stability': 0.0,
            'best_params': best_params
        }
    
    def _rl_optimization(self, model, loss_function, data, **kwargs):
        """å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–"""
        # ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–
        n_episodes = kwargs.get('n_episodes', 100)
        
        # å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆå‚æ•°è°ƒæ•´ï¼‰
        actions = [
            {'lr': 0.001, 'momentum': 0.9},
            {'lr': 0.01, 'momentum': 0.8},
            {'lr': 0.0001, 'momentum': 0.95},
            {'lr': 0.005, 'momentum': 0.85}
        ]
        
        best_action = None
        best_reward = float('-inf')
        
        for episode in range(n_episodes):
            # é€‰æ‹©åŠ¨ä½œ
            action = random.choice(actions)
            
            # åº”ç”¨åŠ¨ä½œ
            optimizer = optim.SGD(model.parameters(), lr=action['lr'], momentum=action['momentum'])
            
            # è®­ç»ƒ
            episode_losses = []
            for _ in range(20):
                optimizer.zero_grad()
                loss = loss_function(model, data)
                loss.backward()
                optimizer.step()
                episode_losses.append(loss.item())
            
            # è®¡ç®—å¥–åŠ±
            reward = -np.mean(episode_losses[-5:])  # è´ŸæŸå¤±ä½œä¸ºå¥–åŠ±
            
            if reward > best_reward:
                best_reward = reward
                best_action = action
            
            if episode % 20 == 0:
                print(f"ğŸ¯ RLä¼˜åŒ– - Episode {episode}, Best Reward: {best_reward:.4f}")
        
        return {
            'strategy': 'reinforcement_learning',
            'final_loss': -best_reward,
            'convergence_speed': n_episodes,
            'stability': 0.0,
            'best_action': best_action
        }
    
    def _update_strategy_performance(self, result):
        """æ›´æ–°ç­–ç•¥æ€§èƒ½"""
        strategy = result['strategy']
        performance = 1.0 / (1.0 + result['final_loss'])  # æŸå¤±è¶Šå°ï¼Œæ€§èƒ½è¶Šå¥½
        
        self.strategy_performance[strategy] = 0.9 * self.strategy_performance[strategy] + 0.1 * performance
    
    def _adapt_strategy(self):
        """è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥"""
        # é€‰æ‹©æ€§èƒ½æœ€å¥½çš„ç­–ç•¥
        best_strategy = max(self.strategy_performance.items(), key=lambda x: x[1])[0]
        
        if best_strategy != self.current_strategy:
            print(f"ğŸ”„ ç­–ç•¥åˆ‡æ¢: {self.current_strategy} -> {best_strategy}")
            self.current_strategy = best_strategy

class AILearningOptimizer:
    """AIå­¦ä¹ å’Œä¼˜åŒ–ç³»ç»Ÿ"""
    
    def __init__(self, model_path=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ§  AIå­¦ä¹ å’Œä¼˜åŒ–ç³»ç»Ÿä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å…ƒå­¦ä¹ ç½‘ç»œ
        self.meta_learner = MetaLearningNetwork().to(self.device)
        self.meta_optimizer = optim.Adam(self.meta_learner.parameters(), lr=0.001)
        
        # è‡ªé€‚åº”ä¼˜åŒ–å™¨
        self.adaptive_optimizer = AdaptiveOptimizer()
        
        # å­¦ä¹ ç»éªŒåº“
        self.experience_database = deque(maxlen=10000)
        self.session_experiences = {}
        
        # æ€§èƒ½è·Ÿè¸ª
        self.performance_history = {
            'loss': [],
            'accuracy': [],
            'learning_rate': [],
            'optimization_time': []
        }
        
        # å­¦ä¹ ç»Ÿè®¡
        self.learning_stats = {
            'total_sessions': 0,
            'total_experiences': 0,
            'average_improvement': 0.0,
            'best_performance': float('inf')
        }
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
            print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒå­¦ä¹ æ¨¡å‹: {model_path}")
    
    def learn_from_experience(self, experience: LearningExperience):
        """ä»ç»éªŒä¸­å­¦ä¹ """
        # å­˜å‚¨ç»éªŒ
        self.experience_database.append(experience)
        
        # æŒ‰ä¼šè¯åˆ†ç»„
        if experience.session_id not in self.session_experiences:
            self.session_experiences[experience.session_id] = []
        self.session_experiences[experience.session_id].append(experience)
        
        # æ›´æ–°ç»Ÿè®¡
        self.learning_stats['total_experiences'] += 1
        
        # å¦‚æœç»éªŒè¶³å¤Ÿï¼Œè¿›è¡Œå…ƒå­¦ä¹ 
        if len(self.experience_database) >= 100:
            self._meta_learn()
    
    def _meta_learn(self):
        """å…ƒå­¦ä¹  - å­¦ä¹ å¦‚ä½•å­¦ä¹ """
        print("ğŸ§  å¼€å§‹å…ƒå­¦ä¹ ...")
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        batch_size = min(32, len(self.experience_database))
        batch = random.sample(self.experience_database, batch_size)
        
        # ç¼–ç ç»éªŒ
        experience_features = []
        learning_targets = []
        
        for exp in batch:
            # æå–ç‰¹å¾
            features = self._encode_experience(exp)
            experience_features.append(features)
            
            # å­¦ä¹ ç›®æ ‡ï¼ˆå¥–åŠ±ï¼‰
            learning_targets.append(exp.reward)
        
        # è½¬æ¢ä¸ºå¼ é‡
        experience_features = torch.stack(experience_features).to(self.device)
        learning_targets = torch.FloatTensor(learning_targets).to(self.device)
        
        # å…ƒå­¦ä¹ è®­ç»ƒ
        self.meta_optimizer.zero_grad()
        meta_features, predicted_lr = self.meta_learner(experience_features)
        
        # è®¡ç®—æŸå¤±
        meta_loss = nn.MSELoss()(meta_features.mean(dim=0), learning_targets.mean().expand(meta_features.size(1)))
        lr_loss = nn.MSELoss()(predicted_lr.squeeze(), torch.ones_like(predicted_lr.squeeze()) * 0.001)
        
        total_loss = meta_loss + 0.1 * lr_loss
        total_loss.backward()
        self.meta_optimizer.step()
        
        print(f"ğŸ§  å…ƒå­¦ä¹ å®Œæˆ - å…ƒæŸå¤±: {meta_loss.item():.4f}, LRæŸå¤±: {lr_loss.item():.4f}")
    
    def _encode_experience(self, experience: LearningExperience) -> torch.Tensor:
        """ç¼–ç å­¦ä¹ ç»éªŒ"""
        features = []
        
        # çŠ¶æ€ç‰¹å¾
        state = experience.state
        features.extend([
            state.get('player_health', 100) / 100.0,
            state.get('player_score', 0) / 1000.0,
            state.get('enemies_killed', 0) / 50.0,
            state.get('survival_time', 0) / 300.0,
        ])
        
        # åŠ¨ä½œç‰¹å¾
        action = experience.action
        features.extend([
            hash(str(action.get('type', ''))) % 100 / 100.0,
            action.get('wave_count', 5) / 10.0,
            action.get('enemies_per_wave', 10) / 20.0,
            action.get('enemy_speed', 3) / 5.0,
        ])
        
        # ç»“æœç‰¹å¾
        outcome = experience.outcome
        features.extend([
            outcome.get('player_survived', True),
            outcome.get('enemies_killed', 0) / 20.0,
            outcome.get('player_damage_taken', 0) / 100.0,
            outcome.get('game_duration', 0) / 300.0,
        ])
        
        # æ—¶é—´ç‰¹å¾
        features.extend([
            experience.timestamp % 86400 / 86400.0,  # ä¸€å¤©å†…çš„æ—¶é—´
            (time.time() - experience.timestamp) / 3600.0,  # ç»éªŒå¹´é¾„ï¼ˆå°æ—¶ï¼‰
        ])
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(features) < 100:
            features.append(0.0)
        
        return torch.FloatTensor(features[:100])
    
    def optimize_game_parameters(self, current_params: Dict[str, Any], 
                               performance_metrics: Dict[str, Any]) -> OptimizationResult:
        """ä¼˜åŒ–æ¸¸æˆå‚æ•°"""
        print("ğŸ”§ å¼€å§‹å‚æ•°ä¼˜åŒ–...")
        
        # åˆ†æå½“å‰æ€§èƒ½
        current_performance = self._evaluate_performance(performance_metrics)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_suggestions = self._generate_optimization_suggestions(
            current_params, performance_metrics
        )
        
        # é€‰æ‹©æœ€ä½³ä¼˜åŒ–
        best_optimization = max(optimization_suggestions, key=lambda x: x['expected_improvement'])
        
        # åº”ç”¨ä¼˜åŒ–
        param_name = best_optimization['parameter']
        old_value = current_params.get(param_name)
        new_value = best_optimization['new_value']
        
        # åˆ›å»ºä¼˜åŒ–ç»“æœ
        result = OptimizationResult(
            parameter_name=param_name,
            old_value=old_value,
            new_value=new_value,
            improvement=best_optimization['expected_improvement'],
            confidence=best_optimization['confidence'],
            reasoning=best_optimization['reasoning']
        )
        
        print(f"ğŸ”§ å‚æ•°ä¼˜åŒ–å®Œæˆ: {param_name} = {old_value} -> {new_value}")
        print(f"   é¢„æœŸæ”¹è¿›: {best_optimization['expected_improvement']:.2f}")
        print(f"   ç½®ä¿¡åº¦: {best_optimization['confidence']:.2f}")
        
        return result
    
    def _evaluate_performance(self, metrics: Dict[str, Any]) -> float:
        """è¯„ä¼°æ€§èƒ½"""
        performance = 0.0
        
        # ç”Ÿå­˜æ—¶é—´
        survival_time = metrics.get('survival_time', 0)
        performance += min(survival_time / 60.0, 1.0) * 0.3
        
        # å‡»æ€æ•ˆç‡
        enemies_killed = metrics.get('enemies_killed', 0)
        performance += min(enemies_killed / 20.0, 1.0) * 0.3
        
        # åˆ†æ•°
        score = metrics.get('score', 0)
        performance += min(score / 1000.0, 1.0) * 0.2
        
        # ä¼¤å®³æ§åˆ¶
        damage_taken = metrics.get('damage_taken', 0)
        performance += max(0, 1.0 - damage_taken / 100.0) * 0.2
        
        return performance
    
    def _generate_optimization_suggestions(self, current_params: Dict[str, Any], 
                                        performance_metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºæ€§èƒ½åˆ†æç”Ÿæˆå»ºè®®
        performance = self._evaluate_performance(performance_metrics)
        
        if performance < 0.3:
            # æ€§èƒ½å·®ï¼Œé™ä½éš¾åº¦
            suggestions.extend([
                {
                    'parameter': 'enemy_speed',
                    'new_value': max(1, current_params.get('enemy_speed', 3) - 1),
                    'expected_improvement': 0.2,
                    'confidence': 0.8,
                    'reasoning': 'ç©å®¶è¡¨ç°ä¸ä½³ï¼Œé™ä½æ•Œæœºé€Ÿåº¦'
                },
                {
                    'parameter': 'enemy_health',
                    'new_value': max(1, current_params.get('enemy_health', 3) - 1),
                    'expected_improvement': 0.15,
                    'confidence': 0.7,
                    'reasoning': 'å‡å°‘æ•Œæœºç”Ÿå‘½å€¼ï¼Œé™ä½éš¾åº¦'
                }
            ])
        elif performance > 0.8:
            # æ€§èƒ½å¥½ï¼Œå¢åŠ æŒ‘æˆ˜
            suggestions.extend([
                {
                    'parameter': 'enemy_speed',
                    'new_value': min(5, current_params.get('enemy_speed', 3) + 1),
                    'expected_improvement': 0.1,
                    'confidence': 0.6,
                    'reasoning': 'ç©å®¶è¡¨ç°ä¼˜ç§€ï¼Œå¢åŠ æŒ‘æˆ˜æ€§'
                },
                {
                    'parameter': 'enemy_count',
                    'new_value': min(50, current_params.get('enemy_count', 30) + 10),
                    'expected_improvement': 0.1,
                    'confidence': 0.6,
                    'reasoning': 'å¢åŠ æ•Œæœºæ•°é‡ï¼Œæµ‹è¯•ç©å®¶æé™'
                }
            ])
        else:
            # æ€§èƒ½ä¸€èˆ¬ï¼Œå¾®è°ƒ
            suggestions.extend([
                {
                    'parameter': 'wave_delay',
                    'new_value': current_params.get('wave_delay', 3.0) * 0.9,
                    'expected_improvement': 0.05,
                    'confidence': 0.5,
                    'reasoning': 'å¾®è°ƒæ•Œæœºç”Ÿæˆé—´éš”ï¼Œä¼˜åŒ–èŠ‚å¥'
                }
            ])
        
        return suggestions
    
    def get_learning_insights(self) -> Dict[str, Any]:
        """è·å–å­¦ä¹ æ´å¯Ÿ"""
        insights = {
            'learning_stats': self.learning_stats,
            'performance_history': self.performance_history,
            'strategy_performance': self.adaptive_optimizer.strategy_performance,
            'current_strategy': self.adaptive_optimizer.current_strategy,
            'experience_diversity': len(set(exp.session_id for exp in self.experience_database)),
            'recent_improvements': self._calculate_recent_improvements()
        }
        
        return insights
    
    def _calculate_recent_improvements(self) -> List[float]:
        """è®¡ç®—æœ€è¿‘çš„æ”¹è¿›"""
        if len(self.performance_history['loss']) < 2:
            return []
        
        recent_losses = self.performance_history['loss'][-10:]
        improvements = []
        
        for i in range(1, len(recent_losses)):
            improvement = recent_losses[i-1] - recent_losses[i]
            improvements.append(improvement)
        
        return improvements
    
    def save_model(self, model_path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'meta_learner': self.meta_learner.state_dict(),
            'meta_optimizer': self.meta_optimizer.state_dict(),
        }, model_path)
        print(f"âœ… å­¦ä¹ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)
        self.meta_learner.load_state_dict(checkpoint['meta_learner'])
        self.meta_optimizer.load_state_dict(checkpoint['meta_optimizer'])
        print(f"âœ… å­¦ä¹ æ¨¡å‹å·²ä» {model_path} åŠ è½½")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºAIå­¦ä¹ å’Œä¼˜åŒ–ç³»ç»Ÿ
    ailo = AILearningOptimizer()
    
    # æ¨¡æ‹Ÿå­¦ä¹ ç»éªŒ
    experience = LearningExperience(
        state={'player_health': 75, 'player_score': 450, 'enemies_killed': 12, 'survival_time': 45},
        action={'type': 'wave', 'wave_count': 5, 'enemies_per_wave': 8, 'enemy_speed': 3},
        reward=0.7,
        next_state={'player_health': 70, 'player_score': 520, 'enemies_killed': 15, 'survival_time': 52},
        outcome={'player_survived': True, 'enemies_killed': 15, 'player_damage_taken': 45, 'game_duration': 52},
        timestamp=time.time(),
        session_id='session_001'
    )
    
    # å­¦ä¹ 
    ailo.learn_from_experience(experience)
    
    # ä¼˜åŒ–å‚æ•°
    current_params = {'enemy_speed': 3, 'enemy_health': 3, 'enemy_count': 30, 'wave_delay': 3.0}
    performance_metrics = {'survival_time': 45, 'enemies_killed': 12, 'score': 450, 'damage_taken': 55}
    
    optimization_result = ailo.optimize_game_parameters(current_params, performance_metrics)
    print(f"ğŸ”§ ä¼˜åŒ–ç»“æœ: {optimization_result}")
    
    # è·å–å­¦ä¹ æ´å¯Ÿ
    insights = ailo.get_learning_insights()
    print(f"ğŸ“Š å­¦ä¹ æ´å¯Ÿ: {insights}")
    
    # ä¿å­˜æ¨¡å‹
    ailo.save_model('./models/ai_learning_optimizer.pth')
