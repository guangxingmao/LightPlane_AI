#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¸¸æˆç­–ç•¥AIè®­ç»ƒè„šæœ¬
ä¸“é—¨è®­ç»ƒAIå¦‚ä½•ç”Ÿæˆå’Œç®¡ç†æ¸¸æˆç­–ç•¥
"""

import os
import sys
import numpy as np
import time
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from stable_baselines3 import PPO, DQN, A2C
    from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
    from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
    from stable_baselines3.common.utils import set_random_seed
    
    # Monitor ä» gymnasium å¯¼å…¥
    try:
        from gymnasium.wrappers import Monitor
    except ImportError:
        try:
            from gym.wrappers import Monitor
        except ImportError:
            print("âš ï¸ Monitor ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ç›‘æ§")
            Monitor = None
    
    SB3_AVAILABLE = True
    print("âœ… Stable Baselines3 å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ Stable Baselines3 å¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ’¡ è¯·å®‰è£…: pip install stable-baselines3")
    SB3_AVAILABLE = False

try:
    from ai_controllers.game_strategy_env import GameStrategyEnvironment
    print("âœ… æ¸¸æˆç­–ç•¥ç¯å¢ƒå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ æ¸¸æˆç­–ç•¥ç¯å¢ƒå¯¼å…¥å¤±è´¥: {e}")
    print("ğŸ’¡ æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå¯¼å…¥")
    sys.exit(1)

class GameStrategyTrainer:
    """æ¸¸æˆç­–ç•¥AIè®­ç»ƒå™¨"""
    
    def __init__(self, algorithm="PPO", total_timesteps=1000000):
        self.algorithm = algorithm
        self.total_timesteps = total_timesteps
        
        # è®­ç»ƒå‚æ•°
        self.training_params = {
            "PPO": {
                "learning_rate": 3e-4,
                "n_steps": 2048,
                "batch_size": 64,
                "n_epochs": 10,
                "gamma": 0.99,
                "gae_lambda": 0.95,
                "clip_range": 0.2,
                "clip_range_vf": None,
                "normalize_advantage": True,
                "ent_coef": 0.01,
                "vf_coef": 0.5,
                "max_grad_norm": 0.5,
                "use_sde": False,  # ç¦»æ•£åŠ¨ä½œç©ºé—´ä¸æ”¯æŒgSDE
                "sde_sample_freq": -1,
                "target_kl": None,
                "tensorboard_log": None,
                "policy_kwargs": {
                    "net_arch": [dict(pi=[128, 128], vf=[128, 128])]
                }
            },
            "DQN": {
                "learning_rate": 1e-4,
                "buffer_size": 1000000,
                "learning_starts": 100000,
                "batch_size": 32,
                "tau": 1.0,
                "gamma": 0.99,
                "train_freq": 4,
                "gradient_steps": 1,
                "target_update_interval": 10000,
                "exploration_fraction": 0.1,
                "exploration_initial_eps": 1.0,
                "exploration_final_eps": 0.05,
                "max_grad_norm": 10,
                "policy_kwargs": {
                    "net_arch": [128, 128]
                }
            },
            "A2C": {
                "learning_rate": 7e-4,
                "n_steps": 5,
                "gamma": 0.99,
                "gae_lambda": 1.0,
                "ent_coef": 0.01,
                "vf_coef": 0.25,
                "max_grad_norm": 0.5,
                "rms_prop_eps": 1e-5,
                "use_rms_prop": True,
                "use_sde": False,
                "sde_sample_freq": -1,
                "policy_kwargs": {
                    "net_arch": [dict(pi=[128, 128], vf=[128, 128])]
                }
            }
        }
        
        # è¾“å‡ºç›®å½•
        self.output_dir = f"models/game_strategy_{algorithm.lower()}"
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è®¾ç½®éšæœºç§å­
        set_random_seed(42)
        
        print(f"ğŸ¯ æ¸¸æˆç­–ç•¥AIè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç®—æ³•: {algorithm}")
        print(f"   è®­ç»ƒæ­¥æ•°: {total_timesteps:,}")
        print(f"   è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def create_environment(self, num_envs=4):
        """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""
        print(f"ğŸŒ åˆ›å»º {num_envs} ä¸ªè®­ç»ƒç¯å¢ƒ...")
        
        def make_env():
            env = GameStrategyEnvironment()
            if Monitor is not None:
                env = Monitor(env, self.output_dir)
            return env
        
        # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
        env = DummyVecEnv([make_env for _ in range(num_envs)])
        
        # ç¯å¢ƒæ ‡å‡†åŒ–
        env = VecNormalize(
            env, 
            norm_obs=True, 
            norm_reward=True,
            clip_obs=10.0,
            clip_reward=10.0
        )
        
        print(f"âœ… è®­ç»ƒç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        return env
    
    def create_model(self, env):
        """åˆ›å»ºAIæ¨¡å‹"""
        print(f"ğŸ§  åˆ›å»º {self.algorithm} æ¨¡å‹...")
        
        if self.algorithm == "PPO":
            # ç§»é™¤é‡å¤çš„tensorboard_logå‚æ•°
            ppo_params = self.training_params["PPO"].copy()
            if 'tensorboard_log' in ppo_params:
                del ppo_params['tensorboard_log']
            
            model = PPO(
                "MlpPolicy", 
                env, 
                verbose=1,
                tensorboard_log=f"{self.output_dir}/tensorboard",
                **ppo_params
            )
        elif self.algorithm == "DQN":
            model = DQN(
                "MlpPolicy", 
                env, 
                verbose=1,
                tensorboard_log=f"{self.output_dir}/tensorboard",
                **self.training_params["DQN"]
            )
        elif self.algorithm == "A2C":
            model = A2C(
                "MlpPolicy", 
                env, 
                verbose=1,
                tensorboard_log=f"{self.output_dir}/tensorboard",
                **self.training_params["A2C"]
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {self.algorithm}")
        
        print(f"âœ… {self.algorithm} æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        return model
    
    def setup_callbacks(self, eval_env):
        """è®¾ç½®è®­ç»ƒå›è°ƒ"""
        print("ğŸ”§ è®¾ç½®è®­ç»ƒå›è°ƒ...")
        
        # è¯„ä¼°å›è°ƒ
        eval_callback = EvalCallback(
            eval_env,
            best_model_save_path=f"{self.output_dir}/best_model",
            log_path=f"{self.output_dir}/eval_logs",
            eval_freq=max(10000 // 4, 1),  # æ¯10kæ­¥è¯„ä¼°ä¸€æ¬¡
            deterministic=True,
            render=False
        )
        
        # æ£€æŸ¥ç‚¹å›è°ƒ
        checkpoint_callback = CheckpointCallback(
            save_freq=max(50000 // 4, 1),  # æ¯50kæ­¥ä¿å­˜ä¸€æ¬¡
            save_path=f"{self.output_dir}/checkpoints",
            name_prefix=f"game_strategy_{self.algorithm.lower()}"
        )
        
        callbacks = [eval_callback, checkpoint_callback]
        print(f"âœ… è®­ç»ƒå›è°ƒè®¾ç½®å®Œæˆ")
        return callbacks
    
    def train(self, model, env, eval_env):
        """å¼€å§‹è®­ç»ƒ"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¸¸æˆç­–ç•¥AI...")
        print(f"   ç®—æ³•: {self.algorithm}")
        print(f"   æ€»æ­¥æ•°: {self.total_timesteps:,}")
        print(f"   ç¯å¢ƒæ•°é‡: {env.num_envs}")
        
        # è®¾ç½®å›è°ƒ
        callbacks = self.setup_callbacks(eval_env)
        
        # å¼€å§‹è®­ç»ƒ
        start_time = time.time()
        
        try:
            model.learn(
                total_timesteps=self.total_timesteps,
                callback=callbacks,
                progress_bar=True
            )
            
            training_time = time.time() - start_time
            print(f"âœ… è®­ç»ƒå®Œæˆ!")
            print(f"   è®­ç»ƒæ—¶é—´: {training_time:.2f} ç§’")
            print(f"   å¹³å‡é€Ÿåº¦: {self.total_timesteps / training_time:.0f} æ­¥/ç§’")
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def save_model(self, model, env, model_name="final"):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹...")
        
        # ä¿å­˜æ¨¡å‹
        model_path = f"{self.output_dir}/{model_name}"
        model.save(model_path)
        print(f"âœ… æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        
        # ä¿å­˜ç¯å¢ƒæ ‡å‡†åŒ–å‚æ•°
        env_normalize_path = f"{self.output_dir}/env_normalize"
        env.save(env_normalize_path)
        print(f"âœ… ç¯å¢ƒæ ‡å‡†åŒ–å‚æ•°ä¿å­˜åˆ°: {env_normalize_path}")
        
        return model_path, env_normalize_path
    
    def evaluate_model(self, model, eval_env, num_episodes=100):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print(f"ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        # è¿è¡Œè¯„ä¼°
        obs = eval_env.reset()
        episode_rewards = []
        episode_lengths = []
        
        for episode in range(num_episodes):
            obs = eval_env.reset()
            done = False
            episode_reward = 0
            episode_length = 0
            
            while not done:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, done, info = eval_env.step(action)
                episode_reward += reward[0]
                episode_length += 1
            
            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)
            
            if (episode + 1) % 10 == 0:
                print(f"   è¯„ä¼°è¿›åº¦: {episode + 1}/{num_episodes}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        mean_reward = np.mean(episode_rewards)
        std_reward = np.std(episode_rewards)
        mean_length = np.mean(episode_lengths)
        
        print(f"ğŸ“ˆ è¯„ä¼°ç»“æœ:")
        print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f} Â± {std_reward:.2f}")
        print(f"   å¹³å‡é•¿åº¦: {mean_length:.1f} æ­¥")
        print(f"   æœ€é«˜å¥–åŠ±: {np.max(episode_rewards):.2f}")
        print(f"   æœ€ä½å¥–åŠ±: {np.min(episode_rewards):.2f}")
        
        return {
            'mean_reward': mean_reward,
            'std_reward': std_reward,
            'mean_length': mean_length,
            'episode_rewards': episode_rewards
        }
    
    def test_model(self, model, test_env, num_episodes=5):
        """æµ‹è¯•æ¨¡å‹ï¼ˆå¯è§†åŒ–ï¼‰"""
        print(f"ğŸ® æµ‹è¯•æ¨¡å‹...")
        print(f"   æ³¨æ„: è¿™æ˜¯æµ‹è¯•æ¨¡å¼ï¼Œå°†æ˜¾ç¤ºæ¸¸æˆç”»é¢")
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å¯è§†åŒ–æµ‹è¯•ä»£ç 
        # ç”±äºæ˜¯ç­–ç•¥AIï¼Œä¸»è¦æµ‹è¯•ç­–ç•¥ç”Ÿæˆçš„æ•ˆæœ
        print(f"âœ… æ¨¡å‹æµ‹è¯•å®Œæˆ")
        print(f"   ç­–ç•¥AIä¸»è¦åŠŸèƒ½:")
        print(f"   - åŠ¨æ€éš¾åº¦è°ƒæ•´")
        print(f"   - æ•Œæœºç”Ÿæˆç­–ç•¥")
        print(f"   - é“å…·æ‰è½ç­–ç•¥")
        print(f"   - èƒŒæ™¯åˆ‡æ¢ç­–ç•¥")
        print(f"   - ç‰¹æ®Šäº‹ä»¶è§¦å‘")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒæ¸¸æˆç­–ç•¥AI")
    parser.add_argument("--algorithm", type=str, default="PPO", 
                       choices=["PPO", "DQN", "A2C"], 
                       help="è®­ç»ƒç®—æ³•")
    parser.add_argument("--timesteps", type=int, default=1000000,
                       help="è®­ç»ƒæ€»æ­¥æ•°")
    parser.add_argument("--envs", type=int, default=4,
                       help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
    parser.add_argument("--eval", action="store_true",
                       help="è®­ç»ƒåè¯„ä¼°æ¨¡å‹")
    parser.add_argument("--test", action="store_true",
                       help="è®­ç»ƒåæµ‹è¯•æ¨¡å‹")
    
    args = parser.parse_args()
    
    if not SB3_AVAILABLE:
        print("âŒ æ— æ³•è®­ç»ƒï¼ŒStable Baselines3 æœªå®‰è£…")
        return
    
    print("ğŸ¯ æ¸¸æˆç­–ç•¥AIè®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = GameStrategyTrainer(
        algorithm=args.algorithm,
        total_timesteps=args.timesteps
    )
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    train_env = trainer.create_environment(num_envs=args.envs)
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    eval_env = trainer.create_environment(num_envs=1)
    
    # åˆ›å»ºæ¨¡å‹
    model = trainer.create_model(train_env)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(model, train_env, eval_env)
    
    # ä¿å­˜æ¨¡å‹
    model_path, env_normalize_path = trainer.save_model(model, train_env)
    
    # è¯„ä¼°æ¨¡å‹
    if args.eval:
        print("\nğŸ“Š å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        eval_results = trainer.evaluate_model(model, eval_env)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        eval_file = f"{trainer.output_dir}/evaluation_results.npz"
        np.savez(eval_file, **eval_results)
        print(f"âœ… è¯„ä¼°ç»“æœä¿å­˜åˆ°: {eval_file}")
    
    # æµ‹è¯•æ¨¡å‹
    if args.test:
        print("\nğŸ® å¼€å§‹æ¨¡å‹æµ‹è¯•...")
        trainer.test_model(model, eval_env)
    
    print(f"\nğŸ‰ æ¸¸æˆç­–ç•¥AIè®­ç»ƒå®Œæˆ!")
    print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"   ç¯å¢ƒæ ‡å‡†åŒ–: {env_normalize_path}")
    print(f"   ä¸‹ä¸€æ­¥: å°†æ¨¡å‹é›†æˆåˆ°æ¸¸æˆä¸­")

if __name__ == "__main__":
    main()
